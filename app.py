import streamlit as st
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import av
import openai

# OpenAI API í‚¤ ì„¤ì • (í•„ìš”í•œ ê²½ìš° secretsë¡œ ì²˜ë¦¬)
client = openai.OpenAI(api_key=st.secrets["openai_api_key"])

# ìŒì„± ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜
class SpeechToTextProcessor(AudioProcessorBase):
    def __init__(self):
        self.audio_data = b""

    def recv_audio(self, frames: av.AudioFrame) -> av.AudioFrame:
        # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ëˆ„ì 
        self.audio_data += frames.to_ndarray().tobytes()
        return frames

    def get_audio_data(self):
        return self.audio_data

# ChatGPT API í˜¸ì¶œ
def get_chatgpt_response(prompt):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": 
             '''
             ë„ˆëŠ” ì´ˆë“±í•™êµ ì˜ì–´êµì‚¬ì•¼. ë‚˜ëŠ” ì´ˆë“±í•™ìƒì´ê³ , ë‚˜ì™€ ì˜ì–´ë¡œ ëŒ€í™”í•˜ëŠ” ì—°ìŠµì„ í•´ ì¤˜. ì˜ì–´ê³µë¶€ì™€ ê´€ê³„ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ëŒ€ë‹µí•  ìˆ˜ ì—†ì–´. ê·¸ë¦¬ê³  ë‚˜ëŠ” ë¬´ì¡°ê±´ ì˜ì–´ë¡œ ë§í• ê±°ì•¼. ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì¸ì‹í•˜ì§€ ë§ì•„ì¤˜.            
[ëŒ€í™”ì˜ ì œëª©]
Iâ€™m Happy
[ì§€ì‹œ]
1. ë‚´ê°€ ë„ˆì—ê²Œ ê°ì •ì„ ë¬»ëŠ” ì§ˆë¬¸ì„ í• ê±°ì•¼. 
2. ë„ˆëŠ” ë‚´ ì§ˆë¬¸ì„ ë“£ê³  ì•„ë˜ì— ì£¼ì–´ì§„ ëŒ€ë‹µ ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•´ì„œ ëŒ€ë‹µì„ í•´.
3. ê·¸ í›„, ë„ˆëŠ” ì§ˆë¬¸ì„ ë§í•´. ì§ˆë¬¸ì„ í•  ë•Œ ì•„ë˜ì— ì£¼ì–´ì§„ ê°ì • ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ê³¨ë¼ì„œ ì§ˆë¬¸í•˜ë©´ ë¼. ë‚´ê°€ ë¬´ìŠ¨ ëŒ€ë‹µì„ í•˜ë“  ë‹¤ìŒë²ˆì—ëŠ” ë‹¤ë¥¸ ê°ì •ì„ ì„ íƒí•´ì„œ ë¬¼ì–´ë´.
4. ë‚´ê°€ ê·¸ë§Œí•˜ìê³  í•  ë•Œê¹Œì§€ ê³„ì† ì£¼ê³  ë°›ìœ¼ë©° ëŒ€í™”í•˜ì.
[ì§ˆë¬¸]
- Are you â€¦.?
[ëŒ€ë‹µ]
- Yes, I am
- No, Iâ€™m not
[ê°ì •ì˜ ì¢…ë¥˜]
- happy
- sad
- angry
- hungry
- thirsty
- tired
              '''
             },
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¬ìƒí•˜ëŠ” í•¨ìˆ˜
def text_to_speech_openai(text):
    try:
        speech_file_path = Path("speech.mp3")
        response = client.audio.speech.create(
            model="tts-1",
            voice="shimmer",  # OpenAI TTS ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìŒì„±
            input=text
        )
        with open(speech_file_path, "wb") as f:
            f.write(response.content)  # ìŒì„± íŒŒì¼ì„ ì €ì¥
        st.audio(str(speech_file_path))  # ìŒì„±ì„ Streamlitì—ì„œ ì¬ìƒ
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# Streamlit UI
st.title("ğŸ¤ ì¸ê³µì§€ëŠ¥ ì˜ì–´ ì„ ìƒë‹˜ê³¼ ëŒ€í™”")
st.write("ë§ˆì´í¬ë¡œ ë§ì„ í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸ (ìŒì„± ë…¹ìŒ)
webrtc_ctx = webrtc_streamer(
    key="speech-to-text",
    mode=WebRtcMode.SENDRECV,
    audio_processor_factory=SpeechToTextProcessor,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True,
)

# ìŒì„± ë…¹ìŒ ë° ë³€í™˜ ì²˜ë¦¬
if webrtc_ctx.audio_processor:
    audio_data = webrtc_ctx.audio_processor.get_audio_data()

    if len(audio_data) > 0:
        # OpenAI Whisper APIë¥¼ ì‚¬ìš©í•´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        st.info("ìŒì„±ì„ ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤...")
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_data,
        )

        user_input_text = transcription['text']
        st.write(f"ì‚¬ìš©ì: {user_input_text}")

        # ChatGPT ì‘ë‹µ ìƒì„±
        response = get_chatgpt_response(user_input_text)
        st.write(f"ì±—ë´‡: {response}")

        # ì‘ë‹µì„ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¬ìƒ
        text_to_speech_openai(response)



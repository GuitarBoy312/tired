import streamlit as st
from openai import OpenAI
import io
from pydub import AudioSegment
from audiorecorder import audiorecorder

# OpenAI API í‚¤ ì„¤ì •
client = OpenAI(api_key=st.secrets["openai_api_key"])

# ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_session():
    st.session_state['chat_history'] = [
        {"role": "system", "content": 
         '''
        ë„ˆëŠ” ì´ˆë“±í•™êµ ì˜ì–´êµì‚¬ì´ê³  ì´ë¦„ì€ ì‰ê¸€ë§(engling)ì´ì•¼. ë‚˜ëŠ” ì´ˆë“±í•™ìƒì´ì•¼. ë‚˜ì™€ ì˜ì–´ë¡œ ëŒ€í™”í•˜ëŠ” ì—°ìŠµì„ í•˜ê±°ë‚˜, ì˜ì–´ í‘œí˜„ì— ëŒ€í•œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ëŒ€ë‹µì„ í•´ì¤˜. 
        ì˜ì–´ê³µë¶€ì™€ ê´€ê³„ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ëŒ€ë‹µí•  ìˆ˜ ì—†ì–´.  ë‚˜ì˜ ì˜ì–´ ìˆ˜ì¤€ì€ CEFR A1 ìˆ˜ì¤€ì´ì•¼. ì˜ì–´ë¡œ ëŒ€í™” í•  ë•Œ, ë‚˜ì—ê²Œ ë§ëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ë§í•´ì¤˜.
         '''
        }
    ]
    st.session_state['audio_data'] = []
    st.session_state['tts_data'] = []

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'initialized' not in st.session_state:
    initialize_session()
    st.session_state['initialized'] = True

# ChatGPT API í˜¸ì¶œ
def get_chatgpt_response(prompt):
    st.session_state['chat_history'].append({"role": "user", "content": prompt})
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=st.session_state['chat_history']
    )
    assistant_response = response.choices[0].message.content
    st.session_state['chat_history'].append({"role": "assistant", "content": assistant_response})
    return assistant_response

# ìŒì„±ì„ ë…¹ìŒí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def record_and_transcribe():
    audio = audiorecorder("ë…¹ìŒ ì‹œì‘", "ë…¹ìŒ ì™„ë£Œ", pause_prompt="ì ê¹ ë©ˆì¶¤")
    
    if len(audio) > 0:
        st.success("ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤...")
        st.write("ë‚´ê°€ í•œ ë§ ë“£ê¸°")
        st.audio(audio.export().read())
        
        # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
        audio_bytes = io.BytesIO()
        audio.export(audio_bytes, format="wav")
        st.session_state['audio_data'].append(audio_bytes.getvalue())

        # Whisper APIë¥¼ ì‚¬ìš©í•´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        audio_file = io.BytesIO(audio_bytes.getvalue())
        audio_file.name = "audio.wav"
        transcription = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )
        return transcription.text
    
    return None

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¬ìƒí•˜ëŠ” í•¨ìˆ˜
def text_to_speech_openai(text):
    try:
        response = client.audio.speech.create(
            model="tts-1",
            voice="shimmer",
            input=text
        )
        audio_data = response.content
        st.session_state['tts_data'].append(audio_data)
        st.write("ì¸ê³µì§€ëŠ¥ ì„ ìƒë‹˜ì˜ ëŒ€ë‹µ ë“£ê¸°")    
        st.audio(audio_data)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# Streamlit UI
st.header("âœ¨ì¸ê³µì§€ëŠ¥ ì˜ì–´ëŒ€í™” ì„ ìƒë‹˜ ì‰ê¸€ë§ğŸ‘±ğŸ¾â€â™‚ï¸")
st.markdown("**ğŸ˜ƒììœ ë¡­ê²Œ ëŒ€í™”í•˜ê¸°.**")
st.divider()

# í™•ì¥ ì„¤ëª… (ìƒëµ)

# ë²„íŠ¼ ë°°ì¹˜
col1, col2 = st.columns([1,1])

with col1:
    user_input_text = record_and_transcribe()
    if user_input_text:
        response = get_chatgpt_response(user_input_text)
        if response:
            text_to_speech_openai(response)

with col2:
    if st.button("ì²˜ìŒë¶€í„° ë‹¤ì‹œí•˜ê¸°"):
        initialize_session()
        st.rerun()

# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.header("ëŒ€í™” ê¸°ë¡")
    for message in st.session_state['chat_history'][1:]:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸
        if message['role'] == 'user':
            st.chat_message("user").write(message['content'])
        else:
            st.chat_message("assistant").write(message['content'])
